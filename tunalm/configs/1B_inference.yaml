# Experiment management
model_name: Llama-3.2-1B-5000-dsus
output_dir: ${experiments_root_dir}/${model_name}
extended_models_dir: /mnt/scratch-artemis/anilkeshwani/models/extended/torchtune/
experiments_root_dir: /mnt/scratch-artemis/anilkeshwani/experiments
experiment_name: null # if null, set to W&B run name

# Model Arguments
model:
  _component_: torchtune.models.llama3_2.llama3_2
  vocab_size: null
  num_layers: 16
  num_heads: 32
  num_kv_heads: 8
  embed_dim: 2048
  max_seq_len: 131072
  attn_dropout: 0.0
  rope_base: 500000
  intermediate_dim: 8192
  norm_eps: 1e-5
  scale_factor: 32

base_vocab_size: 128000 # base vocabulary size Llama 3: 100k (tiktoken) + 28k (targeting multilingual support)
n_special_tokens: 256 # (int) # hard-coded in the torchtune Llama 3 impl.
n_dsus: 5000 # (int)

# Tokenizer (Llama3Tokenizer)
tokenizer:
  path: ${extended_models_dir}/${model_name}/original/tokenizer.model
  max_seq_len: null
  prompt_template: null

# Test Data - NOTE `shuffle` and `packed` are not supported for test inference
data:
  test:
    _component_: torchtune.datasets.text_completion_dataset
    source: anilkeshwani/MLS_english_train_strat_sample_aligned_hubert_interleaved # specified as a HF dataset
    column: text
    split: dev
    add_eos: True

seed: null
batch_size: 4

compile: False # TODO remove if unused

# Training environment
device: cuda

# Reduced precision
dtype: bf16

# Checkpointing
checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: ${extended_models_dir}/${model_name}
  checkpoint_files: [hf_model_0001_0.pt]
  model_type: LLAMA3_2
  output_dir: null # if null, set to ${experiments_root_dir}/${model_name}/${experiment_name}/${checkpoints}
  adapter_checkpoint: null
  recipe_checkpoint: null
  resume_from_checkpoint: False
  safe_serialization: False
