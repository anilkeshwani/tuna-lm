# Experiment management
output_dir: null # user-specified at runtime

# Model Arguments
model:
  _component_: torchtune.models.llama3_2.llama3_2
  vocab_size: null
  num_layers: 16
  num_heads: 32
  num_kv_heads: 8
  embed_dim: 2048
  max_seq_len: 131072
  attn_dropout: 0.0
  rope_base: 500000
  intermediate_dim: 8192
  norm_eps: 1e-5
  scale_factor: 32

base_vocab_size: 128000 # base vocabulary size Llama 3: 100k (tiktoken) + 28k (targeting multilingual support)
n_special_tokens: 256 # (int) # hard-coded in the torchtune Llama 3 impl.
n_dsus: 5000 # (int)

# Test data
# NOTE `shuffle` and `packed` are not supported for test inference
# TODO add a separate config for ASR. Will require alignment of LS {clean, other}
#      provide these as HF datasets - specify "test" split
data:
  test:
    _component_: torchtune.datasets.text_completion_dataset
    source: anilkeshwani/MLS_english_train_strat_sample_aligned_hubert_interleaved # specified as a HF dataset
    column: text
    split: dev # TODO set to test
    add_eos: False # NOTE set add_eos to False as we want to *generate*; both in continuation (CPT) and ASR (SFT) cases

checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: null # user-specified at runtime
  checkpoint_files: [hf_model_0001_0.pt]
  output_dir: ${output_dir}/checkpointer # TODO
  model_type: LLAMA3_2
  adapter_checkpoint: null
  recipe_checkpoint: null
  resume_from_checkpoint: False
  safe_serialization: False

batch_size: 4
device: cuda
dtype: fp32 # dtype: bf16 # NOTE running inference at fp32
seed: 1234

# Tokenizer arguments (Llama3Tokenizer)
tokenizer:
  # _component_
  path: null # user-specified at runtime
  max_seq_len: null
  prompt_template: null

# Generation arguments; defaults taken from gpt-fast
prompt:
  system: null
  user: "Tell me a joke."
max_new_tokens: 500
temperature: 0.6 # 0.8 and 0.6 are popular values to try
top_k: 300

enable_kv_cache: True
#
# quantizer: null # quantizer removed
