# Experiment management
model_name: Llama-3.2-1B-5000-dsus
experiment_name: null # if null, set to W&B run name - NOTE kept this for validation as well for now; remove if wanted
# Checkpoints
checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: ???
  checkpoint_files: null
  model_type: LLAMA3_2
  output_dir: /mnt/scratch-artemis/anilkeshwani/tuna-lm/_output_dir # dummy directory; required by HF checkpointer
  adapter_checkpoint: null
  recipe_checkpoint: null
  resume_from_checkpoint: False
  safe_serialization: False
_config_json_path_hack: /mnt/scratch-artemis/anilkeshwani/experiments/Llama-3.2-1B-5000-dsus/playful-morning-102-id_rq5tmfca/checkpoints/config.json
# Model Arguments
model:
  _component_: torchtune.models.llama3_2.llama3_2 # replaced llama3_2_1b with generic Llama 3.2 function
  vocab_size: null # (int) # NOTE DO NOT SET -> set dynamically as base_vocab_size + n_special_tokens + n_dsus
  # Llama 3.2 1B model architecture spec.
  num_layers: 16 # (int)
  num_heads: 32 # (int)
  num_kv_heads: 8 # (int)
  embed_dim: 2048 # (int)
  max_seq_len: 131072 # (int)
  attn_dropout: 0.0 # (float)
  rope_base: 500000 # (int)
  intermediate_dim: 8192 # (Optional[int])
  norm_eps: 1e-5 # (float)
  scale_factor: 32 # (int)
base_vocab_size: 128000 # base vocabulary size Llama 3: 100k (tiktoken) + 28k (targeting multilingual support)
n_special_tokens: 256 # (int) # hard-coded in the torchtune Llama 3 impl.
n_dsus: 5000 # (int)
# Tokenizer - no _component_ specified (use setup_llama3_tokenizer); special_tokens removed (set dynamically)
tokenizer:
  path: /mnt/scratch-artemis/anilkeshwani/models/extended/torchtune/Llama-3.2-1B-5000-dsus/original/tokenizer.model
  max_seq_len: null # set to 128000000 when packing? (max seq. length per `llama model describe -m Llama3.2-1B`)
  prompt_template: null
# Data
data:
  dev:
    _component_: torchtune.datasets.text_completion_dataset
    source: anilkeshwani/MLS_english_train_strat_sample_aligned_hubert_interleaved # specified as a HF dataset
    column: text
    split: dev
    add_eos: True
    shuffle: False
    packed: False
seed: null
# Optimization
batch_size: 4
# Loss
loss:
  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
# Performance optimizations
compile: False # set it to True for better memory and performance
# Training environment
device: cuda
# Reduced precision
dtype: fp32
